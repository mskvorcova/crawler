# Multithreaded Map-Reduce Crawler

## Описание

Этот проект реализует **многопоточный crawler** файловой системы в стиле **map-reduce**, способный масштабируемо обрабатывать и агрегировать данные, прочитанные из JSON-файлов. Он построен на основе интерфейсов `Pool` и `Crawler`, что позволяет отделить логику многопоточности от логики конкретных операций.

Проект поддерживает параллельное:
- обход дерева файлов (List),
- чтение и преобразование содержимого файлов (Transform),
- агрегацию результатов (Accumulate).

## Интерфейсы

### Pool

Интерфейс `Pool` предоставляет базовую абстракцию для трёх операций:

- **Transform** — применяет трансформер к каждому элементу входного канала;
- **Accumulate** — аггрегирует значения с помощью аккумулятора;
- **List** — расширяет элементы рекурсивно с помощью searcher-функции.

### Crawler

Интерфейс `Crawler` реализует операцию `Collect`, которая использует `Pool` для запуска полного конвейера: поиск файлов → чтение/обработка → аккумуляция. 

Все операции используют воркеры и уважают `context.Context`, обеспечивая корректное завершение работы при отмене.

## Конфигурация

```go
type Configuration struct {
    SearchWorkers      int // количество воркеров для обхода файлов
    FileWorkers        int // количество воркеров для обработки файлов
    AccumulatorWorkers int // количество воркеров для агрегации
}
```

## Поток выполнения

1. `List` обходит директорию, находя все файлы;
2. `Transform` читает каждый файл и десериализует его содержимое, применяя трансформацию;
3. `Accumulate` собирает все промежуточные значения в финальный результат;
4. Все промежуточные результаты комбинируются с помощью `Combiner`.

## Пример использования

```go
res, err := crawler.Collect(
    ctx,
    os.DirFS("."),
    "data",
    Configuration{SearchWorkers: 10, FileWorkers: 5, AccumulatorWorkers: 3},
    myAccumulator,
    myCombiner,
)
```

## Особенности

- Использование **небуферизированных каналов**;
- Правильная обработка отмены контекста (`context.Context`);
- Поддержка **параллельной обработки** на всех этапах;
- Чтение файлов происходит с использованием интерфейса `fs.FileSystem`;
- Ошибки при десериализации файлов обрабатываются внутри `Crawler`.
